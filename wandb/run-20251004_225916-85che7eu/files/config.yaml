_wandb:
    value:
        cli_version: 0.22.0
        e:
            5px1xir2xfpwxtlumwhstowoelamtuyp:
                apple:
                    ecpuCores: 4
                    gpuCores: 8
                    memoryGb: 16
                    name: Apple M1
                    pcpuCores: 4
                    ramTotalBytes: "17179869184"
                    swapTotalBytes: "26843545600"
                args:
                    - configs/GPT-tiny.yaml
                codePath: src/lm/train.py
                codePathLocal: src/lm/train.py
                cpu_count: 8
                cpu_count_logical: 8
                disk:
                    /:
                        total: "994662584320"
                        used: "167974014976"
                email: tejasgoy@andrew.cmu.edu
                executable: /Users/tejas/miniconda3/bin/python
                git:
                    commit: bcde0b067779b7fa9b1b19d37f5fb946cc5ce7b7
                    remote: git@github.com:tejas-goyal/11-667_F25_hw2.git
                host: Tejass-MacBook-Pro.local
                memory:
                    total: "17179869184"
                os: macOS-15.3.1-arm64-arm-64bit-Mach-O
                program: /Users/tejas/Documents/personal/11667/11-667_F25_hw2/src/lm/train.py
                python: CPython 3.13.5
                root: /Users/tejas/Documents/personal/11667/11-667_F25_hw2
                startedAt: "2025-10-05T02:59:16.714431Z"
                writerId: 5px1xir2xfpwxtlumwhstowoelamtuyp
        m: []
        python_version: 3.13.5
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 16
                - 61
            "4": 3.13.5
            "5": 0.22.0
            "12": 0.22.0
            "13": darwin-arm64
batch_size:
    value: 32
device:
    value: auto
grad_accumulation_steps:
    value: 1
max_lr:
    value: 0.0005
min_lr:
    value: 0.0001
model_config:
    value:
        n_embd: 32
        n_head: 2
        n_layer: 2
        n_positions: 128
num_training_steps:
    value: 2000
num_warmup_steps:
    value: 10
output_dir:
    value: outputs/GPT-tiny
seq_len:
    value: 128
tokenizer_encoding:
    value: gpt2
