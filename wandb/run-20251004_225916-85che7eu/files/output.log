model parameters = 2M
Your model is [1;36m6.[0m6MB. This should be within the 100MB limit of Gradescope.
train dataset tokens = 520M
train FLOPs = [1;36m8.13e+13[0m
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [18:05<00:00,  1.84it/s, train loss=7.20, TFLOPS=0.1]
model saved to outputs/GPT-tiny/model.pt
evaluating..: 1220it [03:12,  6.34it/s]
evaluation results: [1m{[0m[32m"val-loss"[0m: [1;36m7.048632548285313[0m, [32m"val-perplexity"[0m: [1;36m1151.2833415068549[0m[1m}[0m
done!
