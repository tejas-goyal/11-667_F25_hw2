model parameters = 4M
Your model is [1;36m17.[0m4MB. This should be within the 100MB limit of Gradescope.
train dataset tokens = 520M
train FLOPs = [1;36m7.07e+16[0m
 29% 188438/650000 [59:55<2:26:47, 52.41it/s, train loss=5.42, TFLOPS=6.4]
Traceback (most recent call last):
  File "/content/drive/MyDrive/11-667_F25_hw2/src/lm/train.py", line 334, in <module>
    main()
  File "/content/drive/MyDrive/11-667_F25_hw2/src/lm/train.py", line 309, in main
    train(
  File "/content/drive/MyDrive/11-667_F25_hw2/src/lm/train.py", line 176, in train
    (loss / grad_accumulation_steps).backward()
  File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
