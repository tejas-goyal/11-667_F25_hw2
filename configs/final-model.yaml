output_dir: outputs/final-model
tokenizer_encoding: gpt2
model_config:
  n_embd: 80
  n_head: 4
  n_positions: 128
  n_layer: 4
device: auto
batch_size: 32
seq_len: 128
num_warmup_steps: 2000
num_training_steps: 650000
grad_accumulation_steps: 1
min_lr: 1e-5
max_lr: 3e-4