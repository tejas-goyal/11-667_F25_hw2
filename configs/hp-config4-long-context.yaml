output_dir: outputs/hp-config4-long-context
tokenizer_encoding: gpt2
model_config:
  n_embd: 48
  n_head: 3
  n_positions: 256
  n_layer: 2
device: auto
batch_size: 32
seq_len: 256
num_warmup_steps: 20
num_training_steps: 2000
grad_accumulation_steps: 1
min_lr: 1e-4
max_lr: 5e-4
