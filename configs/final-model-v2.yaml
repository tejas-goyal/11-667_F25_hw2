output_dir: outputs/final-model-v2
tokenizer_encoding: gpt2
model_config:
  n_embd: 256
  n_head: 8
  n_positions: 256
  n_layer: 8
device: auto
batch_size: 128
seq_len: 256
num_warmup_steps: 600
num_training_steps: 12000
grad_accumulation_steps: 2
max_lr: 0.0005
min_lr: 3.0e-5