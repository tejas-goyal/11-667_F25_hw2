{
    "stdout_visibility": "visible",
    "tests": [
        {
            "score": 5,
            "status": "passed",
            "max_score": 5,
            "name": "test_sequential_batch_sampler",
            "output": "",
            "visibility": "visible"
        },
        {
            "score": 5,
            "status": "passed",
            "max_score": 5,
            "name": "test_cosine_lr_schedule",
            "output": "",
            "visibility": "visible"
        },
        {
            "score": 5,
            "status": "passed",
            "max_score": 5,
            "name": "test_compute_language_modeling_loss",
            "output": "",
            "visibility": "visible"
        },
        {
            "score": 0,
            "status": "passed",
            "max_score": 0,
            "name": "test_train_sanity_check",
            "output": "",
            "visibility": "visible"
        },
        {
            "score": 0,
            "status": "failed",
            "max_score": 5,
            "name": "test_random_batch_sampler",
            "output": "    @max_score(5)\n    def test_random_batch_sampler():\n        tokens = torch.arange(100000)\n        batch_size, seq_len = 32, 256\n        sampler = random_batch_sampler(tokens, \"cpu\", batch_size, seq_len)\n    \n        for _ in range(100):\n            # check the output size is right\n            sample = next(sampler)\n            assert sample.shape == torch.Size([batch_size, seq_len])\n    \n            # check every sequence looks like [x, x+1, x+2, ...]\n            sample_diff = sample - sample[:, [0]]\n            assert torch.all(sample_diff == torch.arange(seq_len))\n    \n        # take a small number of tokens and verify all of them are sampled\n        tokens = torch.arange(7)\n        sampler = random_batch_sampler(tokens, \"cpu\", 64, 4)\n    \n        number_set = set()\n        for _ in range(100):\n            sample = next(sampler).flatten().tolist()\n            number_set |= set(sample)\n    \n>       assert number_set == set(range(7))\nE       AssertionError: assert {0, 1, 2, 3, 4, 5} == {0, 1, 2, 3, 4, 5, ...}\nE         \nE         Extra items in the right set:\nE         \u001b[0m\u001b[94m6\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\nE         \nE         Full diff:\nE         \u001b[0m\u001b[90m \u001b[39;49;00m {\u001b[90m\u001b[39;49;00m\nE         \u001b[90m \u001b[39;49;00m     0,\u001b[90m\u001b[39;49;00m...\nE         \nE         ...Full output truncated (7 lines hidden), use '-vv' to show\nNone\ntests/test_train.py:46: AssertionError",
            "visibility": "visible"
        },
        {
            "score": 0,
            "status": "failed",
            "max_score": 5,
            "name": "test_train",
            "output": "    @max_score(5)\n    def test_train():\n        \"\"\"Fit the model on a trivially learnable dataset - checks drop in loss\"\"\"\n        device = \"cpu\"\n    \n        # initialize tokenizer and model\n        n_vocab = 8\n        n_embd = 16\n        n_head = 8\n        n_positions = 16\n        n_layer = 2\n    \n        num_warmup_steps = 10\n        num_training_steps = 300\n        min_lr = 1e-4\n        max_lr = 5e-4\n        grad_accumulation_steps = 2\n        train_batch_size = 4\n        seq_len = 4\n    \n        model = DecoderLM(n_vocab, n_embd, n_head, n_positions, n_layer).to(device)\n    \n        # create a synthetic dataset for testing\n        train_tokens = val_tokens = torch.arange(n_vocab)\n    \n        train_sampler = random_batch_sampler(\n            train_tokens, device, train_batch_size, seq_len\n        )\n        val_sampler = sequential_batch_sampler(val_tokens, device, 2, seq_len)\n    \n        # prepare optimizer and lr schedule\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=0.0,\n            betas=(0.9, 0.95),\n            fused=device == \"cuda\",\n        )\n        lr_schedule = cosine_lr_schedule(\n            num_warmup_steps, num_training_steps, min_lr, max_lr\n        )\n        autocast = nullcontext()\n        # training\n        model.train()\n        train(\n            model,\n            train_sampler,\n            optimizer,\n            lr_schedule,\n            autocast,\n            num_training_steps,\n            grad_accumulation_steps,\n        )\n    \n        # evaluation\n        model.eval()\n        eval_results = evaluate(model, val_sampler, autocast)\n>       assert eval_results[\"val-loss\"] <= 1.3, \"failed to fit\"\nE       AssertionError: failed to fit\nE       assert 2.1187498569488525 <= 1.3\nNone\ntests/test_train.py:169: AssertionError",
            "visibility": "visible"
        }
    ]
}